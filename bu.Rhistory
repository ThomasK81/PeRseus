setGeneric("spell", function(words, suggests = FALSE, speller = getSpeller()) {
# Note this dispatches to aspell. It is just an alias.
standardGeneric("aspell")
})
setOldClass(c("file", "connection"))
setOldClass(c("url", "connection"))
setOldClass(c("textConnection", "connection"))
setMethod("aspell", "connection",
function(words, suggests = FALSE,  speller = getSpeller())
{
h = if(suggests) DocSpeller() else collectWords()
spellDoc(words, speller = speller)
})
setMethod("aspell", "ANY",
function(words, suggests = FALSE,  speller = getSpeller())
{
vals = lapply(as.character(words), function(w) .Call("Raspell_spell", speller@ref, w, as.logical(suggests), PACKAGE = "Aspell"))
if(!suggests)
vals = unlist(vals)
names(vals) = words
vals
})
saveWordLists =
function(speller)
{
.Call("Raspell_saveWordLists", speller@ref, PACKAGE = "Aspell")
}
addToList =
function(words, speller, session = TRUE)
{
.Call("Raspell_addToList", speller@ref, as.character(words), as.logical(session), PACKAGE = "Aspell")
}
addCorrection =
function(speller, ..., .words, .pairs)
{
if(!missing(.pairs)) {
if(length(.pairs) %%2 != 0)
stop("Need even number of words for mis-spelled-spelled pairs")
idx = seq(1, by = 2, length = length(.pairs)/2)
.words = .pairs[idx + 1]
names(.words) = .pairs[idx]
} else  if(missing(.words)) {
.words = list(...)
ids = names(.words)
}
if(any(names(.words) == ""))
stop("All words must have a name")
.Call("Raspell_storeReplacement", speller@ref, names(.words), as.character(.words), PACKAGE = "Aspell")
}
clearSession =
function(speller)
{
.Call("Raspell_clearSession", speller@ref, PACKAGE = "Aspell")
}
WordListEnum = c(personal = 1, main = 2, session = 3)
getWordList =
function(speller = getSpeller(), which = names(WordListEnum))
{
if(is.character(which)) {
which = pmatch(which, names(WordListEnum))
if(any(is.na(which)))
stop("Invalid value of which in getWordList")
}
if( any(!(which %in% WordListEnum) ))
stop("Invalid integer value for which in getWordList")
tmp = lapply(which, function(i) .Call("Raspell_getWordList", speller@ref, as.integer(i), PACKAGE = "Aspell"))
if(length(which) == 1)
tmp[[1]]
else {
names(tmp) = names(WordListEnum)[which]
tmp
}
}
# Convenience methods
setMethod("$", "AspellSpeller",
function(x, name) {
if(name %in% c("spell", "aspell", "suggest")) {
return(
function(words, suggests = name == "suggests") {
aspell(words, suggests, x)
})
} else if(!is.na(pmatch(name, "config")))
return(getConfig(x))
else if(name %in% names(WordListEnum))
return(getWordList(x, name))
else if(name == "save") {
return(function() saveWordLists(x))
}
stop("Unrecognized instruction ", name, " for AspellSpeller")
})
setMethod("$<-", "AspellSpeller",
function(x, name, value) {
if( name %in% c("session", "personal"))
addToList(value, x, session = (name == "session"))
else if(name == "correct") {
addCorrection(x, .words = value)
} else
stop("Only recognize session and personal as fields to set for AspellSpeller.")
x
})
aspell("love")
b
inspect(b)
require(tm)
inspect(b)
inspect(a)
load("~/Dropbox/PythonRData/boom6.RData")
inspect(a)
inspect(b)
inspect(LetterCorpus)
load("~/Dropbox/PythonRData/boom5.RData")
load("~/Dropbox/PythonRData/boom4.RData")
load("~/Dropbox/PythonRData/woopie.RData")
load("~/Dropbox/PythonRData/Versuch2.RData")
a
inpsect(a)
inspect(a)
inspect(colenso)
ColensoWork <- tm_map(colenso, removeNumbers)
inspect(ColensoWork)
aspell(ColensoWork)
ColensoWork <- tm_map(ColensoWork, gsub, pattern=',', replacement=' ')
inspect(ColensoWork)
ColensoWork <- tm_map(ColensoWork, gsub, pattern='.', replacement=' ')
inspect(ColensoWork)
ColensoWork <- tm_map(colenso, removeNumbers)
inspect(ColensoWork)
ColensoWork <- tm_map(ColensoWork, gsub, pattern=".", replacement=' ')
inspect(ColensoWork)
ColensoWork <- tm_map(colenso, removeNumbers)
inspect(ColensoWork)
errorlist <- aspell(ColensoWork)
nrow(errorlist)
ColensoWork <- tm_map(ColensoWork, removePunctuation)
errorlist <- aspell(ColensoWork)
nrow(errorlist)
ColensoWork <- tm_map(colenso, removeNumbers)
ColensoWork <- tm_map(ColensoWork, gsub, pattern=[:punct:], replacement=' ')
ColensoWork <- tm_map(ColensoWork, gsub, pattern="[:punct:]", replacement=' ')
inspect(ColensoWork)
ColensoWork <- tm_map(colenso, removeNumbers)
inspect(ColensoWork)
errorlist <- aspell(ColensoWork)
nrow(errorlist)
tt <- table(df$Original)
tt
tt <- table(errorlist$Original)
tt
tt < 3
tt < 50
tt < 100
tt < 900
tt < 99000
tt > 99000
tt < 99000
tt < 3
tt < 900
tt < 1
tt < 2
tt > 3
tt > 900
tt > 1
tt > 10
tt[tt > 10]
names(tt[tt > 5])
names(tt[tt > 10])
AHA <- names(tt[tt > 10])
AHA
Viw(AHA)
View(AHA)
ColensoWork <- tm_map(ColensoWork, gsub, pattern="aftn", replacement="afternoon")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Auckld", replacement="Auckland")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="contg", replacement="contingent")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Decr", replacement="December")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="disappd", replacement="disappointed")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="dr", replacement="dear")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Dvk", replacement="Dannevirke")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="earthq", replacement="earthquake")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Engld", replacement="England")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Exhibn", replacement="Exhibition")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="faithy", replacement="faithfully")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Feby", replacement="February")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Geologl", replacement="Geological")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Hepaticæ", replacement="Hepatica")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="howr", replacement="however")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="informn", replacement="informed")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Isld", replacement="Island")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Jany", replacement="January")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="meetg", replacement="meeting")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="mentd", replacement="mentioned")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="morng", replacement="morning")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="mths", replacement="months")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Novr", replacement="November")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Octr", replacement="October")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Parlt", replacement="Parliament")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Presbytn", replacement="Presbytarian")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="respg", replacement="responding")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="retd", replacement="returned")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="retg", replacement="returning")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Revd", replacement="Reverend")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="sd", replacement="send")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Septr", replacement="September")
errorlist <- aspell(ColensoWork)
nrow(errorlist)
tt <- table(errorlist$Original)
rr
tt
names(tt[tt > 100])
ColensoWork <- tm_map(ColensoWork, gsub, pattern="spns", replacement="specimen")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="vols", replacement="volumes")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Wdv", replacement="Woodville")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Wednesenday", replacement="Wednesday")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Wgn", replacement="Wellington")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="yesty", replacement="yesterday")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Tuesenday", replacement="Tuesday")
inspect(ColensoWork)
ColensoWork2 <- ColensoWork
ColensoWork <- tm_map(ColensoWork, gsub, pattern=" th.", replacement=" ")
ColensoWork
inspect(ColensoWork)
errorlist <- aspell(ColensoWork)
nrow(errorlist)
ColensoWork <- ColensoWork2
errorlist <- aspell(ColensoWork)
nrow(errorlist)
tt <- table(errorlist$Original)
names(tt[tt > 100])
names(tt[tt > 1000])
names(tt[tt > 500])
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Thursenday", replacemen2t="Thursday")
ColensoWork <- tm_map(ColensoWork, gsub, pattern="Thursenday", replacement="Thursday")
inspect(ColensoWork)
LC_MESSAGES
test <- c("labour")
apsell(test)
aspell(test)
test <- "test"
aspell(test)
aspell()
aspell(a)
errors <- aspell(a)
tt <- table(errors$Original)
nrow(tt)
coffee_beans
version()
version
version
q()
q()
load("~/OneDrive/GithubProjects/PeRseus/bu.RData")
## setwd, modify according to your needs
setwd("~/OneDrive/GithubProjects/PeRseus")
## libraries needed
library(tm)
library(XML)
library(RCurl)
library(plyr)
library(lda)
library(LDAvis)
library(compiler)
## User settings:
K <- 12
G <- 5000
alpha <- 0.02
eta <- 0.02
seed <- 37
terms_shown <- 40
swLatin <- TRUE
swEnglish <- FALSE
swGreek <- FALSE
swAdditional <- TRUE
language <- "Latin" # (Greek, Persian, Arabic, Latin)
requestURN <- "urn:cts:latinLit:phi1056.phi001"
capabilities_URL <- "http://www.perseus.tufts.edu/hopper/CTS?request=GetCapabilities"
baseURL <- "http://www.perseus.tufts.edu/hopper/CTS?request=GetPassage&urn="
reffURL <- "http://www.perseus.tufts.edu/hopper/CTS?request=GetValidReff&urn="
morpheusURL <- "https://services.perseids.org/bsp/morphologyservice/analysis/word?word="
searchterms <- ""
## read in some stopwords:
stopwords_english <- stopwords("SMART")
stopwords_latin <- c("ab", "ac", "ad", "adhic", "aliqui", "aliquis", "an", "ante", "apud", "at", "atque", "aut", "autem", "cum", "cur", "de", "deinde", "dum", "ego", "enim", "ergo", "es", "est", "et", "etiam", "etsi", "ex", "fio", "haud", "hic", "iam", "idem", "igitur", "ille", "in", "infra", "inter", "interim", "ipse", "is", "ita", "magis", "modo", "mox", "nam", "ne", "nec", "necque", "neque", "nisi", "non", "nos", "o", "ob", "per", "possum", "post", "pro", "quae", "quam", "quare", "qui", "quia", "quicumque", "quidem", "quilibet", "quis", "quisnam", "quisquam", "quisque", "quisquis", "quo", "quoniam", "sed", "si", "sic", "sive", "sub", "sui", "sum", "super", "suus", "tam", "tamen", "trans", "tu", "tum", "ubi", "uel", "uero", "ut", "t", "cos2", "coepio", "sum", "edo")
stopwords_greek <- c("μή", "ἑαυτοῦ", "ἄν", "ἀλλ’", "ἀλλά", "ἄλλοσ", "ἀπό", "ἄρα", "αὐτόσ", "δ’", "δέ", "δή", "διά", "δαί", "δαίσ", "ἔτι", "ἐγώ", "ἐκ", "ἐμόσ", "ἐν", "ἐπί", "εἰ", "εἰμί", "εἴμι", "εἰσ", "γάρ", "γε", "γα^", "ἡ", "ἤ", "καί", "κατά", "μέν", "μετά", "μή", "ὁ", "ὅδε", "ὅσ", "ὅστισ", "ὅτι", "οὕτωσ", "οὗτοσ", "οὔτε", "οὖν", "οὐδείσ", "οἱ", "οὐ", "οὐδέ", "οὐκ", "περί", "πρόσ", "σύ", "σύν", "τά", "τε", "τήν", "τῆσ", "τῇ", "τι", "τί", "τισ", "τίσ", "τό", "τοί", "τοιοῦτοσ", "τόν", "τούσ", "τοῦ", "τῶν", "τῷ", "ὑμόσ", "ὑπέρ", "ὑπό", "ὡσ", "ὦ", "ὥστε", "ἐάν", "παρά", "σόσ")
# stopwords_arabic and stopwords_persian are currently based on frequency only. I welcome pointers to stopword lists for Classical Arabic and Persian
## Decide which set of stopwords
stop_words <- stopwords_latin
# Enable JIT-compiling
enableJIT(3)
### Functions:
## Take the terms from a word list and put them into a format needed by the LDA package
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))}
## Replace word-token with lemmata-vector
lemmatiser <- function(x){
lemmatised <- stem_dictionary[[x]]
return(lemmatised)}
## Choose lemma from each lemmata-vector based on frequency of that lemma in the research corpus
choose_lemma <- function(x){
lemma <- names(which(NumberOccurrences[x]==max(NumberOccurrences[x])))
if (length(lemma)==1) {return(lemma)
}
else {return (x[1])}
}
### parsing the XML in R is a bit of a pain. I am happy for suggestions to make this more efficient!
XMLminer <- function(x){
xname <- xmlName(x)
xattrs <- xmlAttrs(x)
c(sapply(xmlChildren(x), xmlValue), name = xname, xattrs)}
XMLpassage1 <-function(xdata){
result <- xmlParse(xdata)
as.data.frame(t(xpathSApply(result, "//*/tei:body", XMLminer)), stringsAsFactors = FALSE)}
XMLpassage2 <-function(xdata){
result <- xmlParse(xdata)
temp.df <- as.data.frame(t(xpathSApply(result, "//*/hdwd", XMLminer)), stringsAsFactors = FALSE)
as.vector(temp.df[['text']])}
### parsing function: Uses Perseids morphology API to retrive vector of lemmata
### two drawbacks: 1. internet problems would not break code (not anymore), but lead to no lemma returned for requested.
### Uses US server in Boston. Quick in Boston very slow from Europe
### Possible solutions: Requesting already parsed data for edition, thus reducing the API requests from n=number of forms in a corpus to n=1.
parsing <- function(x){
word_form <- x
URL <- paste(morpheusURL, word_form, "&lang=lat&engine=morpheuslat", sep = "")
message(round((match(word_form, corpus_words)-1)/length(corpus_words)*100, digits=2), "% processed. Checking ", x," now.")
URLcontent <- tryCatch({
getURLContent(URL)},
error = function(err)
{tryCatch({
Sys.sleep(0.1)
message("Try once more")
getURLContent(URL)},
error = function(err)
{message("Return original value: ", word_form)
return(word_form)
})
})
if (URLcontent == "ServerError") {
lemma <- x
message(x, " is ", lemma)
return(lemma)}
else {
lemma <- if (is.null(XMLpassage2(URLcontent)) == TRUE) {
lemma <- x
message(x, " is ", lemma)
return(lemma)}
else {lemma <- tryCatch({XMLpassage2(URLcontent)},
error = function(err) {
message(x, " not found. Return original value.")
lemma <- "NotFound1"
message(x, " is ", lemma)
return(lemma)})
lemma <- gsub("[0-9]", "", lemma)
lemma <- tolower(lemma)
lemma <- unique(lemma)
if (nchar(lemma) == 0) {
lemma <- x
message(x, " is ", lemma)
return(lemma)}
else {
message(x, " is ", lemma)
return(lemma)
}
}
}
}
### quick helper functions for vector splitting
first_element <- function(x){
first_element <- head(x, n=1)
return(first_element)}
last_element <- function(x){
last_element <- tail(x, n=1)
return(last_element)}
### find out how topic similarity of citable units
### comparing the mean deviation of theta-values for each topic
is_similar <- function(x) {
check <- all.equal(theta.frame[which(theta.frame[,1] == first_element(unlist(x))),], theta.frame[which(theta.frame[,1] == last_element(unlist(x))),]) # comparing with all.equal
result <- mean(as.numeric(sub(".*?difference: (.*?)", "\\1", check)[3:length(check)]))
return(result)
} # produces NA if compared with itself
### building test matrix to compare a sentence with all other sentences in the corpus
build_test <- function(x){
test_cases <- as.character(AblAbsCaesar[,1]) [! as.character(AblAbsCaesar[,1]) %in% x]
first_column <- rep(x, length(test_cases))
test_matrix <- matrix(nrow=length(test_cases), ncol = 2)
test_matrix[,1] <- first_column
test_matrix[,2] <- test_cases
return(test_matrix)
}
### Import corpus from CTS repository
## Fetch Reffs for CTS Repository
t1 <- Sys.time()
message("Retrieve Reffs for ", requestURN)
URL <- paste(reffURL, requestURN, sep = "")
URLcontent <- getURLContent(URL)
reffs <- unlist(strsplit(URLcontent, split="<urn>|</urn>"))
reffs <- reffs[2:length(reffs)]
reffs <- reffs[seq(1, length(reffs), 2)]
t2 <- Sys.time()
time_ref <- t2 - t1
urls <- paste(baseURL, reffs, sep = "")
## Fetch Text from CTS Repository
capabilities_URL
capabilities_URL <- "http://cts.perseids.org/api/cts?request=GetCapabilities"
capabilities_URL <- "http://cts.perseids.org/api/cts?request=GetCapabilities"
baseURL <- "http://cts.perseids.org/api/cts?request=GetPassage&urn="
reffURL <- "http://cts.perseids.org/api/cts?request=GetValidReff&urn="
URL <- paste(capabilities_URL)
URLcontent <- getURLContent(URL)
URLcontent
URLcontent <- getURLContent(capabilities_URL, .opts=curlOptions(followlocation=TRUE))
URLcontent
what <- xmlToList(URLcontent)
what[1]
what[2]
what[2][1]
what[2][0]
what[2][2]
what[2][[1]]
what2 <_ what[2]
what2 <- what[2]
unlist(what2)[1]
unlist(what2)[2]
unlist(what2)[3]
unlist(what2)[4]
what3 <- as.data.frame(what2)
what3 <- data.frame(what2)
what3 <- table(what2)
is.vector(what2)
is.character(what2)
is.integer(what2)
is.atomic(what2)
what2[[1]]
what2[[2]]
what <- xmlToList(what2)
what <- xmlToList(what2[2])
URLcontent
what2 <- ldply(xmlToList(what), data.frame)
URLcontent
xmlfile <- xmlTreeParse(URLcontent)
class(xmlfile)
xmltop <- xmlRoot(xmlfile)
what <- xmlSApply(xmltop, function(x) xmlSApply(x, xmlValue))
what_df <- data.frame(t(what),row.names=NULL)
View(what_df)
doc <- xmlTreeParse(xmlfile, useInternalNodes = TRUE)
doc <- xmlTreeParse(what_df[:2], useInternalNodes = TRUE)
what_df[1]
what_df[1:1]
what_df[2:1]
doc <- xmlTreeParse(what_df[2:1], useInternalNodes = TRUE)
doc <- xmlTreeParse(unlist(what_df[2:1]), useInternalNodes = TRUE)
doc <- xmlTreeParse(xmlfile, useInternalNodes = TRUE)
doc <- xmlTreeParse(xmlfile)
xmltop <- xmlRoot(xmlfile)
print(xmltop)[1]
print(xmltop)[2]
print(xmltop)[3]
print(xmltop)
plantcat <- xmlSApply(xmltop, function(x) xmlSApply(x, xmlValue))
plantcat
plantcat[1]
plantcat[2]
plantcat[3]
plantcat_df <- data.frame(t(plantcat),row.names=NULL)
View(plantcat_df)
xmlnodes <- xmlTreeParse(URLcontent, getDTD = F)
xmlnodes[1]
rootnode <- xmlRoot(xmlnodes)
xmlChildren(rootnode)
xmlChildren(rootnode)[1]
xmlChildren(rootnode)[2]
what <- xmlToList(xmlChildren(rootnode)[2])
xmlToList(xmlChildren(rootnode)[2][1]
]
xmlToList(xmlChildren(rootnode)[2]
xmlChildren(rootnode)[2][1]
xmlChildren(rootnode)[2][2]
xmlChildren(rootnode)[2][3]
xmlChildren(rootnode)[2]
xmlChildren(rootnode)[3]
xmlChildren(rootnode)[4]
xmlChildren(rootnode)[2]
class(xmlChildren(rootnode)[2])
umlist(xmlChildren(rootnode)[2])[1]
unlist(xmlChildren(rootnode)[2])[1]
unlist(xmlChildren(rootnode)[2])[2]
unlist(xmlChildren(rootnode)[2])[3]
unlist(xmlChildren(rootnode)[2])[4]
unlist(xmlChildren(rootnode)[2])[5]
unlist(xmlChildren(rootnode)[2])[6]
unlist(xmlChildren(rootnode)[2])[7]
unlist(xmlChildren(rootnode)[2])[8]
unlist(xmlChildren(rootnode)[2])[9]
unlist(xmlChildren(rootnode)[2])[10]
unlist(xmlChildren(rootnode)[2])[11]
unlist(xmlChildren(rootnode)[2])[12]
unlist(xmlChildren(rootnode)[2])[13]
xmlChildren(xmlChildren(rootnode))
xmlChildren(xmlChildren(rootnode[2]))
unlist(xmlChildren(rootnode)[2])["reply.children.TextInventory.children.textgroup.attributes.urn"]
unlist(xmlChildren(rootnode)[2])["reply.children.TextInventory.children.textgroup.children.work.attributes.urn"]
unlist(xmlChildren(rootnode)[2])
what <- unlist(xmlChildren(rootnode)[2])
grep('urn:', what, value=TRUE)
what2 <- unname(grep('urn:', what, value=TRUE))
what2
nameswhat <- names(what)
nameswhat
head(nameswhat, n = 15)
head(nameswhat, n = 100)
save.image("~/OneDrive/GithubProjects/PeRseus/bu.RData")
savehistory("~/OneDrive/GithubProjects/PeRseus/bu.Rhistory")
